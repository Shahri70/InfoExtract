{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0icDG3gbm0R",
        "outputId": "d94c184e-9685-4738-fb94-aa217fdbca4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping umap as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping umap-learn as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting umap-learn\n",
            "  Downloading umap-learn-0.5.4.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.8/90.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.11.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.2.2)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.56.4)\n",
            "Collecting pynndescent>=0.5 (from umap-learn)\n",
            "  Downloading pynndescent-0.5.10.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.66.1)\n",
            "Requirement already satisfied: tbb>=2019.0 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (2021.10.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (67.7.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.2.0)\n",
            "Building wheels for collected packages: umap-learn, pynndescent\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.4-py3-none-any.whl size=86770 sha256=a1712eb838ec88386069c61f4834109a8d5cee964a44aad2570468fbe71ba1ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/66/29/199acf5784d0f7b8add6d466175ab45506c96e386ed5dd0633\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.10-py3-none-any.whl size=55615 sha256=357ec92baaac4c81511a052451551e6bfb3e39d839bfc4ba606a93bc3057feab\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/38/5d/f60a40a66a9512b7e5e83517ebc2d1b42d857be97d135f1096\n",
            "Successfully built umap-learn pynndescent\n",
            "Installing collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.10 umap-learn-0.5.4\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "!pip uninstall umap -y\n",
        "!pip uninstall umap-learn -y\n",
        "!pip install umap-learn\n",
        "import umap as umap\n",
        "\n",
        "root = \"/content/drive\"\n",
        "drive.mount(root,  force_remount=False)\n",
        "url_dict = {\n",
        "            'RoBERTa-DE-EN' : 'T-Systems-onsite/cross-en-de-roberta-sentence-transformer',\n",
        "            'USE_Multilingual':'distiluse-base-multilingual-cased-v1'\n",
        "}\n",
        "\n",
        "#@markdown  #Global Parameters\n",
        "\n",
        "model_type = 'RoBERTa-DE-EN' #@param ['RoBERTa-DE-EN', 'USE_Multilingual']\n",
        "\n",
        "model_name = url_dict[model_type]\n",
        "data_name = 'data_'+model_type+'.pkl'\n",
        "embeddings_name = 'embeddings50_'+model_type\n",
        "name = \"LUSIR_Data_Exploration_7-5-21\" #@param {type:'string'}\n",
        "labeled_data_name = 'data_'+model_type\n",
        "\n",
        "\n",
        "#@markdown path_name specifies the folder where everything is saved.\n",
        "path_name = \"Output LUSIR/\" #@param {type:'string'}\n",
        "path = root+\"/My Drive/\"+path_name\n",
        "\n",
        "name_path = path+name+\"/\"\n",
        "output_path = name_path+\"out/\"\n",
        "model_path = name_path+\"model/\"\n",
        "data_path = path+\"preprocessed/\"\n",
        "cluster_path = root+\"/My Drive/\"+\"with_label/50word_with_topic_llama2Model_finale.csv\"\n",
        "data = pd.read_csv(cluster_path)\n",
        "embeddings = np.load(data_path+embeddings_name+'.npy', mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\n",
        "embeddings_reduced = pickle.load(open(model_path+name+\"finale_reducer.pkl\", \"rb\"))\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(embeddings_reduced)\n",
        "embeddings_reduced = scaler.transform(embeddings_reduced)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8Uup99EbY5Q"
      },
      "source": [
        "### **Classification_SVM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhUmIq3KbNAR",
        "outputId": "93c6c66d-087f-42db-d669-e502fcefc7eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 0.974323386537127\n",
            "Linear Kernel Accuracy: 0.9826509368494102\n",
            "Polynomial Kernel Accuracy: 0.9201943095072866\n",
            "RBF Kernel Accuracy: 0.9583622484385843\n",
            "Sigmoid Kernel Accuracy: 0.9174184594031922\n",
            "Linear Kernel classification_report:               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      1.00      0.92        17\n",
            "           1       0.50      1.00      0.67         1\n",
            "           2       0.00      0.00      0.00         0\n",
            "           4       0.82      1.00      0.90         9\n",
            "           5       1.00      1.00      1.00        56\n",
            "           6       1.00      1.00      1.00         2\n",
            "           7       1.00      1.00      1.00         9\n",
            "           9       1.00      1.00      1.00         1\n",
            "          10       1.00      0.89      0.94         9\n",
            "          11       1.00      0.96      0.98        27\n",
            "          12       1.00      1.00      1.00         1\n",
            "          13       1.00      1.00      1.00        24\n",
            "          14       0.00      0.00      0.00         0\n",
            "          16       1.00      1.00      1.00        26\n",
            "          17       0.00      0.00      0.00         1\n",
            "          18       1.00      1.00      1.00        21\n",
            "          19       1.00      1.00      1.00        25\n",
            "          20       0.00      0.00      0.00         0\n",
            "          21       0.67      1.00      0.80         2\n",
            "          22       1.00      1.00      1.00        32\n",
            "          23       1.00      1.00      1.00         1\n",
            "          24       1.00      1.00      1.00        31\n",
            "          25       1.00      1.00      1.00         4\n",
            "          26       1.00      0.95      0.97        20\n",
            "          27       0.00      0.00      0.00         1\n",
            "          28       1.00      1.00      1.00         1\n",
            "          29       1.00      1.00      1.00         1\n",
            "          31       1.00      1.00      1.00         3\n",
            "          32       0.00      0.00      0.00         1\n",
            "          33       1.00      1.00      1.00         2\n",
            "          34       0.00      0.00      0.00         2\n",
            "          35       0.96      1.00      0.98        27\n",
            "          36       1.00      1.00      1.00        63\n",
            "          38       1.00      1.00      1.00         1\n",
            "          39       1.00      1.00      1.00        15\n",
            "          40       0.00      0.00      0.00         0\n",
            "          41       1.00      1.00      1.00        16\n",
            "          43       1.00      1.00      1.00         6\n",
            "          44       0.94      1.00      0.97        16\n",
            "          46       0.00      0.00      0.00         0\n",
            "          47       1.00      1.00      1.00        21\n",
            "          48       1.00      0.96      0.98        23\n",
            "          49       1.00      1.00      1.00         1\n",
            "          50       1.00      0.75      0.86         4\n",
            "          51       1.00      1.00      1.00         5\n",
            "          52       1.00      1.00      1.00        66\n",
            "          53       1.00      0.94      0.97        17\n",
            "          55       1.00      1.00      1.00        29\n",
            "          56       0.00      0.00      0.00         0\n",
            "          59       1.00      1.00      1.00        29\n",
            "          60       1.00      1.00      1.00         5\n",
            "          61       1.00      1.00      1.00         1\n",
            "          62       1.00      1.00      1.00         1\n",
            "          64       1.00      1.00      1.00        43\n",
            "          65       1.00      1.00      1.00        32\n",
            "          66       0.83      1.00      0.91         5\n",
            "          67       0.80      1.00      0.89         8\n",
            "          68       1.00      1.00      1.00         3\n",
            "          69       1.00      1.00      1.00        36\n",
            "          70       1.00      1.00      1.00         1\n",
            "          71       1.00      1.00      1.00        20\n",
            "          72       1.00      1.00      1.00        24\n",
            "          74       0.00      0.00      0.00         2\n",
            "          75       1.00      1.00      1.00        26\n",
            "          76       0.00      0.00      0.00         1\n",
            "          78       1.00      1.00      1.00        29\n",
            "          79       0.00      0.00      0.00         1\n",
            "          82       1.00      1.00      1.00         1\n",
            "          84       1.00      1.00      1.00        23\n",
            "          86       0.00      0.00      0.00         0\n",
            "          87       0.00      0.00      0.00         0\n",
            "          88       1.00      1.00      1.00        26\n",
            "          89       0.00      0.00      0.00         1\n",
            "          90       1.00      1.00      1.00         2\n",
            "          91       1.00      1.00      1.00         2\n",
            "          93       0.89      1.00      0.94         8\n",
            "          94       1.00      1.00      1.00         7\n",
            "          95       0.00      0.00      0.00         0\n",
            "          99       1.00      1.00      1.00         1\n",
            "         101       0.00      0.00      0.00         1\n",
            "         102       0.00      0.00      0.00         0\n",
            "         103       1.00      1.00      1.00        23\n",
            "         104       1.00      1.00      1.00        37\n",
            "         106       0.00      0.00      0.00         0\n",
            "         107       1.00      1.00      1.00        24\n",
            "         108       1.00      0.96      0.98        23\n",
            "         113       1.00      1.00      1.00        32\n",
            "         116       1.00      0.96      0.98        24\n",
            "         117       1.00      1.00      1.00        15\n",
            "         118       1.00      1.00      1.00        22\n",
            "         120       1.00      1.00      1.00        28\n",
            "         121       1.00      1.00      1.00        16\n",
            "         122       0.00      0.00      0.00         1\n",
            "         123       1.00      1.00      1.00        25\n",
            "         124       1.00      1.00      1.00         4\n",
            "         125       0.00      0.00      0.00         1\n",
            "         126       1.00      1.00      1.00        21\n",
            "         127       0.00      0.00      0.00         1\n",
            "         128       1.00      1.00      1.00         2\n",
            "         129       1.00      1.00      1.00        35\n",
            "         130       1.00      1.00      1.00        24\n",
            "         132       1.00      1.00      1.00        22\n",
            "         133       1.00      1.00      1.00         1\n",
            "         134       1.00      1.00      1.00         2\n",
            "         137       0.00      0.00      0.00         0\n",
            "         138       1.00      1.00      1.00        28\n",
            "         140       0.00      0.00      0.00         1\n",
            "         142       1.00      1.00      1.00        27\n",
            "         143       0.00      0.00      0.00         1\n",
            "         145       1.00      0.95      0.98        22\n",
            "\n",
            "    accuracy                           0.98      1441\n",
            "   macro avg       0.75      0.76      0.75      1441\n",
            "weighted avg       0.98      0.98      0.98      1441\n",
            "\n",
            "Polynomial Kernel classification_report:               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.94      0.97        17\n",
            "           1       0.00      0.00      0.00         1\n",
            "           4       0.82      1.00      0.90         9\n",
            "           5       0.90      1.00      0.95        56\n",
            "           6       0.00      0.00      0.00         2\n",
            "           7       0.00      0.00      0.00         9\n",
            "           9       0.00      0.00      0.00         1\n",
            "          10       0.10      1.00      0.18         9\n",
            "          11       0.96      0.93      0.94        27\n",
            "          12       0.00      0.00      0.00         1\n",
            "          13       1.00      1.00      1.00        24\n",
            "          16       1.00      1.00      1.00        26\n",
            "          17       0.00      0.00      0.00         1\n",
            "          18       1.00      0.95      0.98        21\n",
            "          19       1.00      1.00      1.00        25\n",
            "          21       0.00      0.00      0.00         2\n",
            "          22       1.00      0.97      0.98        32\n",
            "          23       0.00      0.00      0.00         1\n",
            "          24       1.00      0.81      0.89        31\n",
            "          25       0.00      0.00      0.00         4\n",
            "          26       1.00      0.95      0.97        20\n",
            "          27       0.00      0.00      0.00         1\n",
            "          28       0.00      0.00      0.00         1\n",
            "          29       0.00      0.00      0.00         1\n",
            "          31       0.00      0.00      0.00         3\n",
            "          32       0.00      0.00      0.00         1\n",
            "          33       0.00      0.00      0.00         2\n",
            "          34       0.00      0.00      0.00         2\n",
            "          35       1.00      0.85      0.92        27\n",
            "          36       1.00      0.98      0.99        63\n",
            "          38       0.00      0.00      0.00         1\n",
            "          39       1.00      1.00      1.00        15\n",
            "          41       0.94      1.00      0.97        16\n",
            "          43       0.50      0.67      0.57         6\n",
            "          44       0.94      1.00      0.97        16\n",
            "          47       0.84      1.00      0.91        21\n",
            "          48       1.00      0.96      0.98        23\n",
            "          49       0.00      0.00      0.00         1\n",
            "          50       1.00      0.75      0.86         4\n",
            "          51       0.00      0.00      0.00         5\n",
            "          52       1.00      0.95      0.98        66\n",
            "          53       1.00      0.94      0.97        17\n",
            "          55       1.00      1.00      1.00        29\n",
            "          59       1.00      0.97      0.98        29\n",
            "          60       0.00      0.00      0.00         5\n",
            "          61       0.00      0.00      0.00         1\n",
            "          62       0.00      0.00      0.00         1\n",
            "          64       1.00      1.00      1.00        43\n",
            "          65       1.00      1.00      1.00        32\n",
            "          66       0.00      0.00      0.00         5\n",
            "          67       0.89      1.00      0.94         8\n",
            "          68       0.00      0.00      0.00         3\n",
            "          69       1.00      1.00      1.00        36\n",
            "          70       0.00      0.00      0.00         1\n",
            "          71       1.00      1.00      1.00        20\n",
            "          72       1.00      0.96      0.98        24\n",
            "          74       0.00      0.00      0.00         2\n",
            "          75       1.00      1.00      1.00        26\n",
            "          76       0.00      0.00      0.00         1\n",
            "          78       1.00      1.00      1.00        29\n",
            "          79       0.00      0.00      0.00         1\n",
            "          82       0.00      0.00      0.00         1\n",
            "          84       1.00      1.00      1.00        23\n",
            "          88       1.00      1.00      1.00        26\n",
            "          89       0.00      0.00      0.00         1\n",
            "          90       0.00      0.00      0.00         2\n",
            "          91       0.00      0.00      0.00         2\n",
            "          93       0.62      1.00      0.76         8\n",
            "          94       0.54      1.00      0.70         7\n",
            "          99       0.00      0.00      0.00         1\n",
            "         101       0.00      0.00      0.00         1\n",
            "         103       1.00      1.00      1.00        23\n",
            "         104       1.00      0.97      0.99        37\n",
            "         107       1.00      0.88      0.93        24\n",
            "         108       1.00      0.96      0.98        23\n",
            "         113       1.00      1.00      1.00        32\n",
            "         116       1.00      0.92      0.96        24\n",
            "         117       0.94      1.00      0.97        15\n",
            "         118       1.00      1.00      1.00        22\n",
            "         120       1.00      1.00      1.00        28\n",
            "         121       1.00      1.00      1.00        16\n",
            "         122       0.00      0.00      0.00         1\n",
            "         123       1.00      1.00      1.00        25\n",
            "         124       1.00      0.75      0.86         4\n",
            "         125       0.00      0.00      0.00         1\n",
            "         126       1.00      1.00      1.00        21\n",
            "         127       0.00      0.00      0.00         1\n",
            "         128       0.00      0.00      0.00         2\n",
            "         129       1.00      1.00      1.00        35\n",
            "         130       1.00      1.00      1.00        24\n",
            "         132       1.00      1.00      1.00        22\n",
            "         133       0.00      0.00      0.00         1\n",
            "         134       0.00      0.00      0.00         2\n",
            "         138       1.00      1.00      1.00        28\n",
            "         140       0.00      0.00      0.00         1\n",
            "         142       1.00      1.00      1.00        27\n",
            "         143       0.00      0.00      0.00         1\n",
            "         145       1.00      0.91      0.95        22\n",
            "\n",
            "    accuracy                           0.92      1441\n",
            "   macro avg       0.54      0.55      0.54      1441\n",
            "weighted avg       0.92      0.92      0.92      1441\n",
            "\n",
            "RBF Kernel classification_report:               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      1.00      0.92        17\n",
            "           1       0.00      0.00      0.00         1\n",
            "           4       0.90      1.00      0.95         9\n",
            "           5       0.92      1.00      0.96        56\n",
            "           6       1.00      1.00      1.00         2\n",
            "           7       0.64      1.00      0.78         9\n",
            "           9       0.00      0.00      0.00         1\n",
            "          10       0.56      1.00      0.72         9\n",
            "          11       0.96      0.96      0.96        27\n",
            "          12       0.00      0.00      0.00         1\n",
            "          13       1.00      1.00      1.00        24\n",
            "          14       0.00      0.00      0.00         0\n",
            "          16       1.00      1.00      1.00        26\n",
            "          17       0.00      0.00      0.00         1\n",
            "          18       0.78      1.00      0.88        21\n",
            "          19       1.00      1.00      1.00        25\n",
            "          21       0.00      0.00      0.00         2\n",
            "          22       1.00      1.00      1.00        32\n",
            "          23       0.00      0.00      0.00         1\n",
            "          24       1.00      1.00      1.00        31\n",
            "          25       0.67      1.00      0.80         4\n",
            "          26       1.00      0.95      0.97        20\n",
            "          27       0.00      0.00      0.00         1\n",
            "          28       0.00      0.00      0.00         1\n",
            "          29       0.00      0.00      0.00         1\n",
            "          31       0.75      1.00      0.86         3\n",
            "          32       0.00      0.00      0.00         1\n",
            "          33       0.50      1.00      0.67         2\n",
            "          34       0.00      0.00      0.00         2\n",
            "          35       0.96      1.00      0.98        27\n",
            "          36       1.00      1.00      1.00        63\n",
            "          38       0.00      0.00      0.00         1\n",
            "          39       1.00      1.00      1.00        15\n",
            "          41       1.00      1.00      1.00        16\n",
            "          43       1.00      1.00      1.00         6\n",
            "          44       1.00      1.00      1.00        16\n",
            "          47       0.91      1.00      0.95        21\n",
            "          48       1.00      1.00      1.00        23\n",
            "          49       0.00      0.00      0.00         1\n",
            "          50       1.00      0.75      0.86         4\n",
            "          51       0.71      1.00      0.83         5\n",
            "          52       1.00      0.97      0.98        66\n",
            "          53       1.00      0.94      0.97        17\n",
            "          55       1.00      1.00      1.00        29\n",
            "          59       1.00      1.00      1.00        29\n",
            "          60       0.00      0.00      0.00         5\n",
            "          61       0.00      0.00      0.00         1\n",
            "          62       0.00      0.00      0.00         1\n",
            "          64       1.00      1.00      1.00        43\n",
            "          65       1.00      1.00      1.00        32\n",
            "          66       0.56      1.00      0.71         5\n",
            "          67       0.67      1.00      0.80         8\n",
            "          68       0.00      0.00      0.00         3\n",
            "          69       1.00      1.00      1.00        36\n",
            "          70       0.00      0.00      0.00         1\n",
            "          71       1.00      1.00      1.00        20\n",
            "          72       1.00      0.96      0.98        24\n",
            "          74       0.00      0.00      0.00         2\n",
            "          75       1.00      1.00      1.00        26\n",
            "          76       0.00      0.00      0.00         1\n",
            "          78       1.00      1.00      1.00        29\n",
            "          79       0.00      0.00      0.00         1\n",
            "          82       0.00      0.00      0.00         1\n",
            "          84       1.00      1.00      1.00        23\n",
            "          88       1.00      1.00      1.00        26\n",
            "          89       0.00      0.00      0.00         1\n",
            "          90       0.00      0.00      0.00         2\n",
            "          91       0.00      0.00      0.00         2\n",
            "          93       0.53      1.00      0.70         8\n",
            "          94       0.70      1.00      0.82         7\n",
            "          99       0.00      0.00      0.00         1\n",
            "         101       0.00      0.00      0.00         1\n",
            "         103       1.00      1.00      1.00        23\n",
            "         104       1.00      1.00      1.00        37\n",
            "         107       1.00      1.00      1.00        24\n",
            "         108       1.00      0.96      0.98        23\n",
            "         113       1.00      1.00      1.00        32\n",
            "         116       1.00      0.92      0.96        24\n",
            "         117       0.94      1.00      0.97        15\n",
            "         118       1.00      1.00      1.00        22\n",
            "         120       1.00      1.00      1.00        28\n",
            "         121       1.00      1.00      1.00        16\n",
            "         122       0.00      0.00      0.00         1\n",
            "         123       1.00      1.00      1.00        25\n",
            "         124       0.80      1.00      0.89         4\n",
            "         125       0.00      0.00      0.00         1\n",
            "         126       1.00      1.00      1.00        21\n",
            "         127       0.00      0.00      0.00         1\n",
            "         128       0.00      0.00      0.00         2\n",
            "         129       1.00      1.00      1.00        35\n",
            "         130       1.00      1.00      1.00        24\n",
            "         132       1.00      1.00      1.00        22\n",
            "         133       0.00      0.00      0.00         1\n",
            "         134       0.00      0.00      0.00         2\n",
            "         138       1.00      1.00      1.00        28\n",
            "         140       0.00      0.00      0.00         1\n",
            "         142       1.00      1.00      1.00        27\n",
            "         143       0.00      0.00      0.00         1\n",
            "         145       1.00      0.91      0.95        22\n",
            "\n",
            "    accuracy                           0.96      1441\n",
            "   macro avg       0.59      0.63      0.60      1441\n",
            "weighted avg       0.94      0.96      0.95      1441\n",
            "\n",
            "Sigmoid Kernel classification_report:               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      1.00      0.79        17\n",
            "           1       0.00      0.00      0.00         1\n",
            "           4       0.80      0.89      0.84         9\n",
            "           5       0.93      0.96      0.95        56\n",
            "           6       1.00      1.00      1.00         2\n",
            "           7       0.56      1.00      0.72         9\n",
            "           9       0.00      0.00      0.00         1\n",
            "          10       0.53      0.89      0.67         9\n",
            "          11       0.96      0.96      0.96        27\n",
            "          12       0.00      0.00      0.00         1\n",
            "          13       1.00      1.00      1.00        24\n",
            "          16       1.00      1.00      1.00        26\n",
            "          17       0.00      0.00      0.00         1\n",
            "          18       0.88      1.00      0.93        21\n",
            "          19       1.00      1.00      1.00        25\n",
            "          21       0.00      0.00      0.00         2\n",
            "          22       0.79      0.72      0.75        32\n",
            "          23       0.00      0.00      0.00         1\n",
            "          24       1.00      0.84      0.91        31\n",
            "          25       0.00      0.00      0.00         4\n",
            "          26       1.00      0.80      0.89        20\n",
            "          27       0.00      0.00      0.00         1\n",
            "          28       0.00      0.00      0.00         1\n",
            "          29       0.00      0.00      0.00         1\n",
            "          31       0.67      0.67      0.67         3\n",
            "          32       0.00      0.00      0.00         1\n",
            "          33       0.00      0.00      0.00         2\n",
            "          34       0.00      0.00      0.00         2\n",
            "          35       0.90      1.00      0.95        27\n",
            "          36       0.91      1.00      0.95        63\n",
            "          38       0.00      0.00      0.00         1\n",
            "          39       1.00      1.00      1.00        15\n",
            "          41       1.00      1.00      1.00        16\n",
            "          43       1.00      0.83      0.91         6\n",
            "          44       1.00      1.00      1.00        16\n",
            "          47       0.76      0.90      0.83        21\n",
            "          48       1.00      1.00      1.00        23\n",
            "          49       0.00      0.00      0.00         1\n",
            "          50       1.00      0.75      0.86         4\n",
            "          51       0.71      1.00      0.83         5\n",
            "          52       0.98      0.92      0.95        66\n",
            "          53       0.84      0.94      0.89        17\n",
            "          55       1.00      1.00      1.00        29\n",
            "          59       0.85      0.79      0.82        29\n",
            "          60       0.00      0.00      0.00         5\n",
            "          61       0.00      0.00      0.00         1\n",
            "          62       0.00      0.00      0.00         1\n",
            "          64       0.66      0.86      0.75        43\n",
            "          65       1.00      1.00      1.00        32\n",
            "          66       1.00      0.40      0.57         5\n",
            "          67       1.00      0.50      0.67         8\n",
            "          68       0.00      0.00      0.00         3\n",
            "          69       1.00      1.00      1.00        36\n",
            "          70       0.00      0.00      0.00         1\n",
            "          71       1.00      1.00      1.00        20\n",
            "          72       1.00      1.00      1.00        24\n",
            "          74       0.00      0.00      0.00         2\n",
            "          75       1.00      1.00      1.00        26\n",
            "          76       0.00      0.00      0.00         1\n",
            "          78       0.83      1.00      0.91        29\n",
            "          79       0.00      0.00      0.00         1\n",
            "          82       0.00      0.00      0.00         1\n",
            "          84       1.00      1.00      1.00        23\n",
            "          88       1.00      1.00      1.00        26\n",
            "          89       0.00      0.00      0.00         1\n",
            "          90       0.00      0.00      0.00         2\n",
            "          91       0.00      0.00      0.00         2\n",
            "          93       0.50      1.00      0.67         8\n",
            "          94       0.58      1.00      0.74         7\n",
            "          99       0.00      0.00      0.00         1\n",
            "         101       0.00      0.00      0.00         1\n",
            "         103       1.00      1.00      1.00        23\n",
            "         104       0.83      0.92      0.87        37\n",
            "         107       1.00      0.96      0.98        24\n",
            "         108       1.00      0.96      0.98        23\n",
            "         113       1.00      1.00      1.00        32\n",
            "         116       0.96      1.00      0.98        24\n",
            "         117       0.93      0.87      0.90        15\n",
            "         118       1.00      0.86      0.93        22\n",
            "         120       1.00      1.00      1.00        28\n",
            "         121       0.94      1.00      0.97        16\n",
            "         122       0.00      0.00      0.00         1\n",
            "         123       1.00      1.00      1.00        25\n",
            "         124       0.67      1.00      0.80         4\n",
            "         125       0.00      0.00      0.00         1\n",
            "         126       0.95      1.00      0.98        21\n",
            "         127       0.00      0.00      0.00         1\n",
            "         128       0.00      0.00      0.00         2\n",
            "         129       0.95      1.00      0.97        35\n",
            "         130       1.00      1.00      1.00        24\n",
            "         132       0.96      1.00      0.98        22\n",
            "         133       0.00      0.00      0.00         1\n",
            "         134       0.00      0.00      0.00         2\n",
            "         138       1.00      1.00      1.00        28\n",
            "         140       0.00      0.00      0.00         1\n",
            "         142       1.00      1.00      1.00        27\n",
            "         143       0.00      0.00      0.00         1\n",
            "         145       1.00      0.91      0.95        22\n",
            "\n",
            "    accuracy                           0.92      1441\n",
            "   macro avg       0.57      0.58      0.57      1441\n",
            "weighted avg       0.90      0.92      0.90      1441\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "y = data['cluster']\n",
        "X = embeddings_reduced\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "linear_svm = SVC(kernel='linear')\n",
        "linear_svm.fit(X_train, y_train)\n",
        "\n",
        "poly_svm = SVC(kernel='poly', degree=3)  # Polynomial kernel of degree 3\n",
        "poly_svm.fit(X_train, y_train)\n",
        "\n",
        "rbf_svm = SVC(kernel='rbf')\n",
        "rbf_svm.fit(X_train, y_train)\n",
        "\n",
        "sigmoid_svm = SVC(kernel='sigmoid')\n",
        "sigmoid_svm.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions using each classifier\n",
        "y_linear_predictions = linear_svm.predict(X_test)\n",
        "y_poly_predictions = poly_svm.predict(X_test)\n",
        "y_rbf_predictions = rbf_svm.predict(X_test)\n",
        "y_sigmoid_predictions = sigmoid_svm.predict(X_test)\n",
        "\n",
        "# Evaluate the performance of each classifier\n",
        "linear_accuracy = accuracy_score(y_test, y_linear_predictions)\n",
        "poly_accuracy = accuracy_score(y_test, y_poly_predictions)\n",
        "rbf_accuracy = accuracy_score(y_test, y_rbf_predictions)\n",
        "sigmoid_accuracy = accuracy_score(y_test, y_sigmoid_predictions)\n",
        "\n",
        "random_forest = RandomForestClassifier(random_state=42)\n",
        "random_forest.fit(X_train, y_train)\n",
        "y_rf_predictions = random_forest.predict(X_test)\n",
        "rf_accuracy = accuracy_score(y_test, y_rf_predictions)\n",
        "print(f\"Random Forest Accuracy: {rf_accuracy}\")\n",
        "\n",
        "print(\"Linear Kernel Accuracy:\", linear_accuracy)\n",
        "print(\"Polynomial Kernel Accuracy:\", poly_accuracy)\n",
        "print(\"RBF Kernel Accuracy:\", rbf_accuracy)\n",
        "print(\"Sigmoid Kernel Accuracy:\", sigmoid_accuracy)\n",
        "\n",
        "print(\"Linear Kernel classification_report:\", classification_report(y_test, y_linear_predictions))\n",
        "print(\"Polynomial Kernel classification_report:\", classification_report(y_test, y_poly_predictions))\n",
        "print(\"RBF Kernel classification_report:\", classification_report(y_test, y_rbf_predictions))\n",
        "print(\"Sigmoid Kernel classification_report:\", classification_report(y_test, y_sigmoid_predictions))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XW9MrW4HlREz"
      },
      "source": [
        "### **Classification_KNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWtCbyrJlU7w",
        "outputId": "078bd950-fa15-443b-8f25-a56026c2fce5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result of KNN:\n",
            "Accuracy : 0.9805690492713394\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "print(\"Result of KNN:\")\n",
        "y = data['cluster']\n",
        "X = embeddings_reduced\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "k = 3  # Number of neighbors to consider\n",
        "knn_model = KNeighborsClassifier(n_neighbors=k)\n",
        "\n",
        "# Train the model on the training data\n",
        "knn_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = knn_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy :\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8VS0J473XO5",
        "outputId": "00e4c812-6f0b-4a4b-8c34-42ac07b6ebca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90\n",
            "Epoch 1/100\n",
            "23/23 - 1s - loss: 4.4711 - accuracy: 0.1223 - val_loss: 3.6868 - val_accuracy: 0.2283 - 836ms/epoch - 36ms/step\n",
            "Epoch 2/100\n",
            "23/23 - 0s - loss: 2.8584 - accuracy: 0.4067 - val_loss: 2.0565 - val_accuracy: 0.5913 - 89ms/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "23/23 - 0s - loss: 1.5058 - accuracy: 0.7420 - val_loss: 1.1147 - val_accuracy: 0.8050 - 112ms/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "23/23 - 0s - loss: 0.8682 - accuracy: 0.8740 - val_loss: 0.7358 - val_accuracy: 0.8994 - 109ms/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "23/23 - 0s - loss: 0.5997 - accuracy: 0.9133 - val_loss: 0.5598 - val_accuracy: 0.9098 - 79ms/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "23/23 - 0s - loss: 0.4636 - accuracy: 0.9244 - val_loss: 0.4571 - val_accuracy: 0.9223 - 79ms/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "23/23 - 0s - loss: 0.3769 - accuracy: 0.9374 - val_loss: 0.3934 - val_accuracy: 0.9417 - 92ms/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "23/23 - 0s - loss: 0.3181 - accuracy: 0.9480 - val_loss: 0.3464 - val_accuracy: 0.9417 - 96ms/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "23/23 - 0s - loss: 0.2739 - accuracy: 0.9523 - val_loss: 0.3122 - val_accuracy: 0.9452 - 85ms/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "23/23 - 0s - loss: 0.2378 - accuracy: 0.9535 - val_loss: 0.2849 - val_accuracy: 0.9514 - 91ms/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "23/23 - 0s - loss: 0.2080 - accuracy: 0.9594 - val_loss: 0.2606 - val_accuracy: 0.9556 - 87ms/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "23/23 - 0s - loss: 0.1829 - accuracy: 0.9632 - val_loss: 0.2415 - val_accuracy: 0.9563 - 83ms/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "23/23 - 0s - loss: 0.1626 - accuracy: 0.9663 - val_loss: 0.2307 - val_accuracy: 0.9604 - 94ms/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "23/23 - 0s - loss: 0.1450 - accuracy: 0.9684 - val_loss: 0.2158 - val_accuracy: 0.9639 - 84ms/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "23/23 - 0s - loss: 0.1292 - accuracy: 0.9724 - val_loss: 0.2062 - val_accuracy: 0.9660 - 75ms/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "23/23 - 0s - loss: 0.1167 - accuracy: 0.9750 - val_loss: 0.1985 - val_accuracy: 0.9702 - 83ms/epoch - 4ms/step\n",
            "Epoch 17/100\n",
            "23/23 - 0s - loss: 0.1058 - accuracy: 0.9752 - val_loss: 0.1927 - val_accuracy: 0.9702 - 99ms/epoch - 4ms/step\n",
            "Epoch 18/100\n",
            "23/23 - 0s - loss: 0.0954 - accuracy: 0.9799 - val_loss: 0.1868 - val_accuracy: 0.9688 - 89ms/epoch - 4ms/step\n",
            "Epoch 19/100\n",
            "23/23 - 0s - loss: 0.0870 - accuracy: 0.9811 - val_loss: 0.1827 - val_accuracy: 0.9702 - 94ms/epoch - 4ms/step\n",
            "Epoch 20/100\n",
            "23/23 - 0s - loss: 0.0786 - accuracy: 0.9827 - val_loss: 0.1789 - val_accuracy: 0.9736 - 96ms/epoch - 4ms/step\n",
            "Epoch 21/100\n",
            "23/23 - 0s - loss: 0.0729 - accuracy: 0.9837 - val_loss: 0.1768 - val_accuracy: 0.9736 - 77ms/epoch - 3ms/step\n",
            "Epoch 22/100\n",
            "23/23 - 0s - loss: 0.0661 - accuracy: 0.9854 - val_loss: 0.1745 - val_accuracy: 0.9757 - 84ms/epoch - 4ms/step\n",
            "Epoch 23/100\n",
            "23/23 - 0s - loss: 0.0599 - accuracy: 0.9863 - val_loss: 0.1717 - val_accuracy: 0.9757 - 88ms/epoch - 4ms/step\n",
            "Epoch 24/100\n",
            "23/23 - 0s - loss: 0.0570 - accuracy: 0.9875 - val_loss: 0.1739 - val_accuracy: 0.9757 - 96ms/epoch - 4ms/step\n",
            "Epoch 25/100\n",
            "23/23 - 0s - loss: 0.0521 - accuracy: 0.9887 - val_loss: 0.1702 - val_accuracy: 0.9792 - 90ms/epoch - 4ms/step\n",
            "Epoch 26/100\n",
            "23/23 - 0s - loss: 0.0483 - accuracy: 0.9896 - val_loss: 0.1708 - val_accuracy: 0.9778 - 92ms/epoch - 4ms/step\n",
            "Epoch 27/100\n",
            "23/23 - 0s - loss: 0.0455 - accuracy: 0.9899 - val_loss: 0.1706 - val_accuracy: 0.9778 - 83ms/epoch - 4ms/step\n",
            "Epoch 28/100\n",
            "23/23 - 0s - loss: 0.0414 - accuracy: 0.9913 - val_loss: 0.1703 - val_accuracy: 0.9792 - 89ms/epoch - 4ms/step\n",
            "Epoch 29/100\n",
            "23/23 - 0s - loss: 0.0386 - accuracy: 0.9924 - val_loss: 0.1703 - val_accuracy: 0.9792 - 95ms/epoch - 4ms/step\n",
            "Epoch 30/100\n",
            "23/23 - 0s - loss: 0.0366 - accuracy: 0.9936 - val_loss: 0.1705 - val_accuracy: 0.9799 - 81ms/epoch - 4ms/step\n",
            "Epoch 31/100\n",
            "23/23 - 0s - loss: 0.0339 - accuracy: 0.9938 - val_loss: 0.1712 - val_accuracy: 0.9799 - 75ms/epoch - 3ms/step\n",
            "Epoch 32/100\n",
            "23/23 - 0s - loss: 0.0321 - accuracy: 0.9934 - val_loss: 0.1722 - val_accuracy: 0.9806 - 92ms/epoch - 4ms/step\n",
            "Epoch 33/100\n",
            "23/23 - 0s - loss: 0.0303 - accuracy: 0.9950 - val_loss: 0.1732 - val_accuracy: 0.9799 - 93ms/epoch - 4ms/step\n",
            "Epoch 34/100\n",
            "23/23 - 0s - loss: 0.0285 - accuracy: 0.9957 - val_loss: 0.1751 - val_accuracy: 0.9806 - 90ms/epoch - 4ms/step\n",
            "Epoch 35/100\n",
            "23/23 - 0s - loss: 0.0266 - accuracy: 0.9969 - val_loss: 0.1735 - val_accuracy: 0.9833 - 101ms/epoch - 4ms/step\n",
            "46/46 [==============================] - 0s 817us/step\n",
            "Neural Network Accuracy: 0.9791811242192922\n"
          ]
        }
      ],
      "source": [
        "#!pip install tensorflow\n",
        "#!pip install --upgrade tensorflow\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "embeddings2=embeddings\n",
        "# Split your data into training and testing sets\n",
        "y = data['cluster']\n",
        "X = embeddings_reduced\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(X_train.shape[1])\n",
        "# Standardize your features (important for neural networks)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Input(shape=(X_train.shape[1],)),  # Input layer\n",
        "    keras.layers.Dense(X_train.shape[1], activation='relu'),    # Hidden layer with 128 units and ReLU activation\n",
        "    keras.layers.Dense(X_train.shape[1]/2, activation='relu'),     # Hidden layer with 64 units and ReLU activation\n",
        "    keras.layers.Dense(len(y.unique()), activation='sigmoid')   # Output layer with 10 units (adjust based on your number of clusters)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=256, validation_split=0.2, verbose=2,\n",
        "        callbacks=[keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)],\n",
        "        validation_data=(X_test, y_test))\n",
        "# Evaluate the model on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_labels = [np.argmax(pred) for pred in y_pred]\n",
        "accuracy = accuracy_score(y_test, y_pred_labels)\n",
        "print(f\"Neural Network Accuracy: {accuracy}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}