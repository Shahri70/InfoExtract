{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFXsrSxJlzCW"
      },
      "source": [
        "#Introduction\n",
        "\n",
        "Welcome to the tutorial on text embeddings with neural language models!\n",
        "\n",
        "Here you will learn how to produce embeddings of a collection of short pieces of text. The embeddings correlate with the semantic meaning of the short pieces of text.\n",
        "\n",
        "This allows us to automatically compare the embeddings of two or more pieces of text to see if they are similar in meaning.\n",
        "\n",
        "The embeddings are produced by a pretrained neural language model. There are different ways to perform pretraining. A common approach is to show the model pairs of sentences that are deemed to be similar. For instance, this could be two sentences that occur in the sam Wikipedia article. The exact pretraining procedure and training data is different for each language model.\n",
        "Different language models are available online, some are even multilingual. Which one we use to generate embeddings can be specified below.\n",
        "\n",
        "Our code is executed on a Virtual Machine (VM) provided by Colab. It has enough processing power and memory to run our code.\n",
        "A VM is just a computer without hardware. It can do almost anything a normal computer can do, but it is only virtual, i.e. it is simulated on another computer or server.\n",
        "Colab VMs use Ubuntu, a Linux-based operating system. Many common libraries we use for data an analysis are already installed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvCIkIPe9B7b"
      },
      "source": [
        "##Imports\n",
        "We need some libraries to run our code. By importing them into our notebook, we can use them.\n",
        "\n",
        "We use the following Libraries:\n",
        "\n",
        "**NumPy (np)** 'is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.' (https://en.wikipedia.org/wiki/NumPy\n",
        "\n",
        "**Pandas (pd)** 'is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.' (https://pandas.pydata.org/) Pandas represents data in a spreadsheet-like format and allows us to manipulate it accordingly.\n",
        "\n",
        "**os** allows is to access the operating system. Will be used to create folders.\n",
        "\n",
        "**sentence_transformers** 'SentenceTransformers is a Python framework for state-of-the-art sentence, text and image embeddings. The initial work is described in our paper Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.\n",
        "You can use this framework to compute sentence / text embeddings for more than 100 languages. These embeddings can then be compared e.g. with cosine-similarity to find sentences with a similar meaning. This can be useful for semantic textual similar, semantic search, or paraphrase mining. The framework is based on PyTorch and Transformers and offers a large collection of pre-trained models tuned for various tasks. Further, it is easy to fine-tune your own models.' (https://www.sbert.net/)\n",
        "\n",
        "\n",
        "\n",
        "**google.colab drive** is used to mount our Google drive folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VG_6Fky9YhZ"
      },
      "source": [
        "# Before we can use sentence_transformers we need to install it via pip ('pip is the package installer for Python.' https://pypi.org/project/pip/)\n",
        "!pip install sentence_transformers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from sentence_transformers import SentenceTransformer,util\n",
        "\n",
        "\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-P4MVydXRmD"
      },
      "source": [
        "##Set Paths\n",
        "\n",
        "First we need to define the file paths.\n",
        "\n",
        "This includes the path to our input folder and our output folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOkvmXgVXT3_"
      },
      "source": [
        "# Define the name of the root folder.\n",
        "root = \"/content/Gdrive\"\n",
        "#@markdown **path_name** specifies the folder where everything is saved.\n",
        "input_path_name = \"Data LUSIR/\" #@param {type:'string'}\n",
        "\n",
        "#@markdown **path_name** specifies the folder where everything is saved.\n",
        "output_path_name = \"Output LUSIR/\" #@param {type:'string'}\n",
        "\n",
        "output_path = root+\"/My Drive/\"+output_path_name\n",
        "\n",
        "# Define the output folder\n",
        "preprocessed_path = output_path+\"preprocessed/\"\n",
        "\n",
        "# Define the input folder\n",
        "data_path = root+\"/My Drive/\"+input_path_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtZoL1-EfP97"
      },
      "source": [
        "##Set File Names\n",
        "\n",
        "Then we need to define, which file we want to use to produce embeddings.\n",
        "\n",
        "The file should be a .pkl-file that represents a Pandas-dataframe. It should have a column \"sentences\" from where the sentences are read that we want to produce embeddings of."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyn-1yjafSxG"
      },
      "source": [
        "#@markdown **input_file_name** specified which file you want to load. It should be the exact name of the file you want to load\n",
        "input_file_name = \"LUSIR_df_speakers_clean_normalized_50sentence(s)_NEW - CORPUS\" #@param {type:'string'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4pdtrbIirPf"
      },
      "source": [
        "## Mount Drive Folder\n",
        "\n",
        "In order to access our data, we need to connect the Colab VM to our Google Drive.\n",
        "\n",
        "A new Colab VM is assigned to us every time we start the Colab notebook.\n",
        "\n",
        "Colab VMs run on one of Google's servers. For security and privacy reasons, it cannot access our data without our permission.\n",
        "\n",
        "Similar to how one would connect a network drive to a computer, we can connect our Google Drive to this Colab VM.\n",
        "\n",
        "Colab uses OAuth2 to authorize the VM to access our Google Drive.\n",
        "\n",
        "1. Execute the cell\n",
        "2. Follow the link in the output.\n",
        "3. Select your Google account\n",
        "4. Confirm access privileges\n",
        "4. Copy the link.\n",
        "5. Paste the link in the input field in the output.\n",
        "6. Press **Enter**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_KtnQV4UOaR"
      },
      "source": [
        "drive.mount(root, force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4woGfi0AjJYJ"
      },
      "source": [
        "## Create Folders\n",
        "\n",
        "Before we can write anything into our output folder, we have to create it.\n",
        "\n",
        "If it already exists, no new directory will be created, but we will get a different output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2d5-Hr-jRLm"
      },
      "source": [
        "paths = [output_path, preprocessed_path]\n",
        "\n",
        "for p in paths:\n",
        "  print(p)\n",
        "  #try to create folders\n",
        "\n",
        "  try:\n",
        "     os.mkdir(p)\n",
        "  except OSError:\n",
        "    print (\"Creation of the directory %s failed\" % p)\n",
        "  else:\n",
        "    print (\"Successfully created the directory %s \" % p)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VlwSfswYj9o"
      },
      "source": [
        "## Set Language model\n",
        "\n",
        "Here, we can specify, which language model we use.\n",
        "\n",
        "**RoBERTa-DE-EN** (*hosted at*: https://huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer, *Paper*: Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L. & Stoyanov, V. (2019). RoBERTa: A Robustly Optimized BERT Pretraining Approach (cite arxiv:1907.11692))\n",
        "\n",
        "**USE_Multilingual** (*hosted at*: sbert.net , *Paper*: Yang, Y., Cer, D.M., Ahmad, A., Guo, M., Law, J., Constant, N., √Åbrego, G., Yuan, S., Tar, C., Sung, Y., Strope, B., & Kurzweil, R. (2020). Multilingual Universal Sentence Encoder for Semantic Retrieval. ACL.)\n",
        "\n",
        "Reimers, Nils & Gurevych, Iryna. (2020). Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TL7Rvq_TD086"
      },
      "source": [
        "url_dict = {\n",
        "            'RoBERTa-DE-EN' : 'T-Systems-onsite/cross-en-de-roberta-sentence-transformer',\n",
        "            'USE_Multilingual':'distiluse-base-multilingual-cased-v1'\n",
        "}\n",
        "\n",
        "#@markdown  #Global Parameters\n",
        "\n",
        "model_type = 'RoBERTa-DE-EN' #@param ['RoBERTa-DE-EN', 'USE_Multilingual']\n",
        "\n",
        "model_name = url_dict[model_type]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryGTfBQoYsLS"
      },
      "source": [
        "## Pandas print options\n",
        "\n",
        "To get more readable outputs when printing our data in Pandas, we have to set some print options."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEH28FctS6SP"
      },
      "source": [
        "#pandas print options\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VO9QOLP_9DxJ"
      },
      "source": [
        "#Load Data\n",
        "\n",
        "To access our data we need to load it into our notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbU3mk83MBIH"
      },
      "source": [
        "# Specify the input file\n",
        "input_file = data_path+input_file_name\n",
        "\n",
        "# Load the data by reading the .pkl-file with Pandas..\n",
        "data = pd.read_pickle(input_file)\n",
        "\n",
        "# Sepcify which column is used to produce embeddings from\n",
        "text_column = 'chunk'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VevHlK2Ve4hJ"
      },
      "source": [
        "# Print the shape of our data (number of rows, number of columns)\n",
        "print(data.shape)\n",
        "\n",
        "# Print the first rows\n",
        "data.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IUWISh1M5nz"
      },
      "source": [
        "# Preprocessing\n",
        "\n",
        "Some preprocessing can be done to produce better embeddings. This step higly depends on the input data. For interviews, we could for example parse out speaker names, such as \\*Interviewer says\\*:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqSh9Outdujo"
      },
      "source": [
        "## Drop unused columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vw3q3rqdtka"
      },
      "source": [
        "data = data.dropna(subset=[text_column]) # drop rows with no content\n",
        "print(data.shape)\n",
        "#data=data.drop(['0'],axis=1) # drop unused columns\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkzSbhgJdz61"
      },
      "source": [
        "##Remove New Line Tag"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIgxEvzcM39J"
      },
      "source": [
        "\n",
        "# Remove new line \\n\n",
        "new_line_pattern = r'\\n'\n",
        "data['chunk'] = data['chunk'].str.replace(new_line_pattern, ' ')\n",
        "print( \"Removed new line \\n\")\n",
        "\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "spacy.cli.download('de_core_news_sm')\n",
        "nlp = spacy.load('de_core_news_sm')"
      ],
      "metadata": {
        "id": "lJ3mGAQGa-IY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "open_stoplist = 'german_stopwords_full_BE_MOD Topics.txt'\n",
        "stopword_path = root+\"/My Drive/Output LUSIR/\"\n",
        "\n",
        "stoplist = open(stopword_path+open_stoplist, encoding='UTF-16', mode='r').read().split()\n",
        "\n",
        "print(stoplist)"
      ],
      "metadata": {
        "id": "LHXXzBpobG2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Remove stop words\n",
        "    filtered_words = [word for word in text.split() if word.lower() not in stoplist]\n",
        "\n",
        "    # Lemmatization\n",
        "    allowed_postags=['NOUN', 'PROPN', 'VERB', 'ADJ', 'ADV']\n",
        "    min_wordlen = 2 #@param {type:\"integer\"}\n",
        "    doc = nlp(text)\n",
        "    lemmatized_words= [token.lemma_ for token in doc if len(token) > min_wordlen and token.pos_ in allowed_postags]\n",
        "\n",
        "    # Join the lemmatized words back into a single string\n",
        "    processed_text = ' '.join(lemmatized_words)\n",
        "    return processed_text\n",
        "\n",
        "data['processed']= data['chunk'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "_xxMzQgmbMAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YxXP5PtAFyV"
      },
      "source": [
        "# Create Text Embeddings with Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7avncQB5aKn"
      },
      "source": [
        "## Load the model and Embed Documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_sStH0bXhFD"
      },
      "source": [
        "###Load Model\n",
        "\n",
        "We use a pretrained model to produce our embeddings.\n",
        "Before we can use the model we have to download it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugb89Xy7XdqR"
      },
      "source": [
        "model = SentenceTransformer(model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9MceMTo70Am"
      },
      "source": [
        "###Embed documents\n",
        "\n",
        "Then we can pass our data into the model and it will return embeddings. This will take some time.\n",
        "\n",
        "For each Sentence (i.e. each row in the data frame) one embedding is generated.\n",
        "Embeddings are just lists of numbers that can be thought of as points in high-dimensional space. Each point represents one text. The closer two points are to each other, the more similar two sentences are."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L80f-f7J72V-"
      },
      "source": [
        "print('Embedding...')\n",
        "embeddings = model.encode(data['processed'], convert_to_tensor=True, batch_size = 128, show_progress_bar = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGNx6LmHI-np"
      },
      "source": [
        "# The embeddings are matrices of numbers of shape (number of sentences, number of embedding dimensions)\n",
        "print(embeddings.shape)\n",
        "embeddings[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZXUfe-dra8k"
      },
      "source": [
        "# Try out the model\n",
        "\n",
        "After we have generated embeddings, we can compare them.\n",
        "\n",
        "Similarity is calculated with the cosine similarity.\n",
        "\n",
        "We can either define our own lists of sentences to compare.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7pHEGLaEl-3"
      },
      "source": [
        "# Two lists of sentences\n",
        "sentences1 = ['The cat is sitting on the roof.',\n",
        "             'A man is playing guitar.',\n",
        "             'The new movie is awesome.',\n",
        "              'I want to wear a hat.',\n",
        "              'this is cool',\n",
        "              'it is cool',\n",
        "              'Hello!, Hello!Hello!Hello!Hello!Hello!',\n",
        "              'Tab'\n",
        "              ]\n",
        "\n",
        "sentences2 = ['Die Katze sitzt auf dem Dach.',\n",
        "              'A woman watches TV.',\n",
        "              'The new movie is so great.',\n",
        "              'Ich trage einen Hut.',\n",
        "              'is this cool',\n",
        "              'it is sad',\n",
        "              'Hi! Hello!Hello!',\n",
        "              'Bat'\n",
        "              ]\n",
        "\n",
        "#Compute embedding for both lists\n",
        "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
        "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
        "\n",
        "#Compute cosine-similarits\n",
        "cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
        "\n",
        "#Output the pairs with their score\n",
        "for i in range(len(sentences1)):\n",
        "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1zAxSbD2H-1"
      },
      "source": [
        "Or compare randomly selected sentences from the LUSIR dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13g7_bENs4QQ"
      },
      "source": [
        "# Single list of sentences\n",
        "number_of_samples = 300 #@param {type:'integer'}\n",
        "\n",
        "#sentences = data['cleaned'][start:start+range_].tolist()\n",
        "\n",
        "sentences = data[text_column]\n",
        "\n",
        "sentences = sentences.sample(n=number_of_samples).tolist()\n",
        "\n",
        "#Compute embeddings\n",
        "embeddings_sentences = model.encode(sentences, convert_to_tensor=True)\n",
        "\n",
        "#Compute cosine-similarities for each sentence with each other sentence\n",
        "cosine_scores = util.pytorch_cos_sim(embeddings_sentences, embeddings_sentences)\n",
        "\n",
        "\n",
        "#Find the pairs with the highest cosine similarity scores\n",
        "pairs = []\n",
        "for i in range(len(cosine_scores)-1):\n",
        "    for j in range(i+1, len(cosine_scores)):\n",
        "        if cosine_scores[i][j] >= .5:\n",
        "          pairs.append({'index': [i, j], 'score': cosine_scores[i][j]})\n",
        "\n",
        "#Sort scores in decreasing order\n",
        "pairs = sorted(pairs, key=lambda x: x['score'], reverse=False)\n",
        "print(len(pairs))\n",
        "for pair in pairs[0:20]:\n",
        "    i, j = pair['index']\n",
        "    print(\"{}\\n{}\\n Score: {:.4f}\\n\\n\".format(sentences[i], sentences[j], pair['score']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0T6C9w-W7YC"
      },
      "source": [
        "#Save Data and Embeddings\n",
        "\n",
        "After the data is preprocessed and embeddings are generated, both are saved to disk for further use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9VS7ChLXB15"
      },
      "source": [
        " data.to_pickle(preprocessed_path+'data_'+model_type+'.pkl')\n",
        " print(data.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9say2_vW5Fi"
      },
      "source": [
        "import numpy as np\n",
        "embeddings = embeddings.to('cpu').detach().numpy()\n",
        "np.save(preprocessed_path+'embeddings_'+model_type+'.npy', embeddings, allow_pickle=True, fix_imports=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}